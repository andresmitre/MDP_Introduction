{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< [In Depth: MDP Wikipedia example](MDP-wikipedia.ipynb) | [In-Dept: MDP Exercise](MDP-2.ipynb) >\n",
    "\n"
    ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Markov decision process (MDP) is a discrete time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker.\n",
    "At each time step, the process is in some state $S$, and the decision maker may choose any action a that is available in state $S$. The process responds at the next time step by randomly moving into a new state $s'$, and giving the decision maker a corresponding reward $R_a(S,S')$.\n",
    "\n",
    "The probability that the process moves into its new state $s'$ is influenced by the chosen action. Specifically, it is given by the state transition function $P_a(S,S')$. Thus, the next state $S'$ depends on the current state $S$ and the decision maker's action $a$. But given $S$ and $a$, it is conditionally independent of all previous states and actions; in other words, the state transitions of an MDP satisfies the Markov property.\n",
    "\n",
    "A Markov decision process is a 4-tuple $(S,A,P_a,R_a)$, where:\n",
    "\n",
    "$S$ is a finite set of states,\n",
    "\n",
    "$A$ is a finite set of actions (alternatively, $A_s$ is the finite set of actions available from state $S$\n",
    "\n",
    "$P_a (S,S') = P_r (S_t+1 = S';  S_t = S , a_t = a )$ is the probability that action $a$ in state $S$ at time $t$ will lead to state $S'$  at time $t + 1$,\n",
    "\n",
    "$R_a (S,S')$ is the immediate reward (or expected immediate reward) received after transitioning from state $S$ to state $S'$, due to action $a$"
]
},
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "The value function for the MDP is defined as:"
    ]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
  "![](images/equation.png)"
 ]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
  "###The policy\n",
  "The objective of this MDP is to find the optimum policy for the reward $+5$ intuitively starting from $S_0$ we can see that the best route to take are the actions $a_1, a_1, a_0$ respectivly.\n",
  "At each time step, the process is in some state $S$, and the decision maker may choose any action a that is available in state $S$. The process responds at the next time step by randomly moving into a new state $s'$, and giving the decision maker a corresponding reward $R_a(S,S')$.\n",
  "\n",
  "Breaking down the problem:\n",
  "\n",
  "Consider we start at the $S_0$, and we have the following vectors:\n",
  "$$V_1=\\begin{pmatrix}S_0 \\\\ S_1 \\\\ S_2 \\end{pmatrix}$$\n",
  "$$V_2=\\begin{pmatrix}S_0 \\\\ S_1 \\\\ S_2 \\end{pmatrix}$$\n",
  "\n",
  "We can either take action $a_0$ and have a probability of 1/2 to go to $S_1$ and other 1/2 to go back to $S_0$.\n",
  "According to the formula above we can get the value function from taking the two actions:and have a probability of 1/2 to go to $S_1$ and other 1/2 to go back to $S_0$.\n",
  "$$V(1)=max_{a} {\\sum_{S'} = P_a(0,1)(R_a(S,S') + gamma V_i(S'))}$$\n",
  "$$a_{0} = {(0.5)(0 + 0.1 (0))} + {(0.5)(0 + 0.1 (0))} = 0$$\n",
  "$$a_{1}= {(0.5)(0 + 0.1 (0))} = 0$$\n",
  "$$V_{2} := v_{i+1(s)} = max_{a} {\\sum_{S'} = P_a(0,1)(R_a(S,S') + gamma V_i(S'))}$$\n",
  ""
]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
  "\n",
  "Consider the next example (figure example from [wikipedia](https://en.wikipedia.org/wiki/Markov_decision_process)):\n",
  "![](images/MDP.png)"
  ]
},
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
